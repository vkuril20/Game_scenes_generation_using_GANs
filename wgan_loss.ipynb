{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42687015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n",
      "g done\n",
      "Starting Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 278>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m b_size \u001b[38;5;241m=\u001b[39m real_cpu\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    298\u001b[0m label_real \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(b_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 299\u001b[0m output_real \u001b[38;5;241m=\u001b[39m \u001b[43mnetD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m errD_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(output_real)\n\u001b[1;32m    301\u001b[0m D_x\u001b[38;5;241m=\u001b[39moutput_real\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.stats import entropy\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.set_device(2)\n",
    "\n",
    "def inception_score(imgs,device,batch_size=64, resize=True, splits=1):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    #imgs_tensor = torch.from_numpy(imgs)\n",
    "\n",
    "    # Move the tensor to the desired device\n",
    "    #imgs_tensor = imgs_tensor.to(device)\n",
    "    imgs = imgs.to(device)\n",
    "    #imgs = imgs_tensor\n",
    "    N = imgs.size(0)\n",
    "    if resize:\n",
    "        imgs = nn.functional.interpolate(imgs, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    #imgs = imgs.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n",
    "\n",
    "    inception_model = models.inception_v3(pretrained=True, transform_input=False).eval().to(device)\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=False).to(device)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch = imgs[i:i + batch_size]\n",
    "        batch = up(batch)\n",
    "        logits = inception_model(batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        kl = probs * (torch.log(probs) - torch.log(torch.tensor(1.0 / probs.size(1)).to(device)))\n",
    "        kl = kl.sum(1)\n",
    "        scores.append(kl.cpu().numpy())\n",
    "\n",
    "    scores = np.concatenate(scores)\n",
    "    scores = np.exp(scores.mean())\n",
    "\n",
    "    if splits > 1:\n",
    "        kl = probs * (torch.log(probs) - torch.log(torch.tensor(1.0 / probs.size(1)).to(device)))\n",
    "        kl = kl.sum(1).detach().cpu().numpy()\n",
    "        split_scores = []\n",
    "        for k in range(splits):\n",
    "            part = kl[k * (N // splits): (k + 1) * (N // splits)]\n",
    "            py = np.exp(part.mean())\n",
    "            split_scores.append(py)\n",
    "        return scores, np.std(split_scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"/home/stud1/Aman/dataset\"\n",
    "\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size =64\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 128\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 128\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 128\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize((128,128)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:1\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:32], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "# plt.savefig('training_images_wgan.png')\n",
    "# plt.close()\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.02.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "#print(netG)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf *16, 1, 4, 1, 0, bias=False),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "    \n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "#print(netD)\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 0.9\n",
    "fake_label = 0.1\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# Training Loop\n",
    "netD.load_state_dict(torch.load('discriminator_weights.pth'))\n",
    "netG.load_state_dict(torch.load('generator_weights.pth'))\n",
    "\n",
    "print('g done')\n",
    "\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "i_score=[]\n",
    "iters = 0\n",
    "clip_value=0.01\n",
    "fid_score_=[]\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Save generator weights\n",
    "    torch.save(netG.state_dict(), 'generator_weights.pth')\n",
    "\n",
    "    # Save discriminator weights\n",
    "    torch.save(netD.state_dict(), 'discriminator_weights.pth')\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    " \n",
    "     # Clip discriminator parameters\n",
    "        # for p in netD.parameters():\n",
    "        #     p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "        # Reset gradients\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # Train with real data\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label_real = torch.ones(b_size, 1).to(device)\n",
    "        output_real = netD(real_cpu)\n",
    "        errD_real = -torch.mean(output_real)\n",
    "        D_x=output_real.mean().item()\n",
    "        \n",
    "        # Train with fake data\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label_fake = torch.zeros(b_size, 1).to(device)\n",
    "        output_fake = netD(fake.detach())\n",
    "        errD_fake = torch.mean(output_fake)\n",
    "        D_G_z1 =output_fake.mean().item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        for p in netD.parameters():\n",
    "            p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        # Reset gradients\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        #fake = netG(noise)\n",
    "\n",
    "        # Compute discriminator output on fake data\n",
    "        output = netD(fake)\n",
    "        errG = -torch.mean(output)\n",
    "\n",
    "        # Backward and optimize\n",
    "        errG.backward()\n",
    "        D_G_z2=output.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i  == len(dataloader)-1:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f\\tD(G(z)):  %.4f'\n",
    "                   % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        #if (iters % 39 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "        if i == len(dataloader)-1:\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "                vutils.save_image(fake, 'Check.png', normalize=True)\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1\n",
    "    with torch.no_grad():\n",
    "        fake = netG(fixed_noise).detach().cpu()\n",
    "        fake_images = vutils.make_grid(fake, normalize=True).numpy()\n",
    "        inception_score_value= inception_score(fake,device, batch_size=64, resize=True, splits=1)\n",
    "        print(\"Num Epochs: {}, inception_score: {:.4f}\".format(epoch,inception_score_value))\n",
    "        i_score.append(inception_score_value)\n",
    "        \n",
    "        fid_score = calculate_fid(real_cpu, fake, batch_size, inception_batch_size=64, device=device)\n",
    "        print(\"Num Epochs: {}, FID: {:.4f}\".format(epoch,fid_score))\n",
    "        fid_score_.append(fid_score)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    file_name = 'Loss_b_{}_i_{}_e_{}_wgan.png'.format(batch_size, image_size, num_epochs)\n",
    "    plt.savefig(file_name,dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    epochs_ = list(range(1, len(i_score) + 1))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Inception Score over Epochs')\n",
    "    plt.plot(epochs_, i_score, marker='o')  # 'o' for circular markers\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Inception Score')\n",
    "    plt.grid(True)\n",
    "    #plt.legend()\n",
    "    file_name = 'Inception_score_b_{}_i_{}_e_{}_wgan.png'.format(batch_size, image_size, num_epochs)\n",
    "    plt.savefig(file_name,dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    with open('Inception_scores_b_{}_i_{}_e_{}_wgan.txt'.format(batch_size, image_size,num_epochs), 'w') as file:\n",
    "        for score in i_score:\n",
    "            file.write(str(score) + '\\n')\n",
    "            \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('fid Score over Epochs')\n",
    "    plt.plot(epochs_, fid_score_, marker='o')  # 'o' for circular markers\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('fid score')\n",
    "    plt.grid(True)\n",
    "    #plt.legend()\n",
    "    file_name = 'fid_score_b_{}_i_{}_e_{}_wgan.png'.format(batch_size, image_size, num_epochs)\n",
    "    plt.savefig(file_name,dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    with open('fid_score_b_{}_i_{}_e_{}_wgan.txt'.format(batch_size, image_size,num_epochs), 'w') as file:\n",
    "        for score in fid_score_:\n",
    "            file.write(str(score) + '\\n')\n",
    "            \n",
    "#     fig = plt.figure(figsize=(12, 12))  # Increase the figure size to 12x12 inches\n",
    "#     plt.axis(\"off\")\n",
    "#     ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\n",
    "#     ani = animation.ArtistAnimation(fig, ims, interval=200, repeat_delay=1000, blit=True)  # Decrease the interval to 200 milliseconds for faster animation\n",
    "#     ani.save('b_{}_i_{}_wgane_e_{}.gif'.format(batch_size, image_size,epoch), writer='imagemagick', dpi=300)  # Include batch_size, image_size, num_epochs, and iterations in the filename, and increase the dpi to 300 for higher quality\n",
    "#     plt.close()\n",
    "\n",
    "    # fig = plt.figure(figsize=(8,8))\n",
    "    # plt.axis(\"off\")\n",
    "    # ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "    # ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "    # ani.save('b_128_i_128_wgan_10000.gif', writer='imagemagick')  # For GIF format\n",
    "    # plt.close()\n",
    "\n",
    "    # HTML(ani.to_jshtml())\n",
    "\n",
    "    # Grab a batch of real images from the dataloader\n",
    "    real_batch = next(iter(dataloader))\n",
    "\n",
    "    # Plot the real images\n",
    "    plt.figure(figsize=(45,45))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Real Images\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    # Plot the fake images from the last epoch\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake Images\")\n",
    "    plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "    file_name = 'Real_vs_fake_b_{}_i_{}_e_{}_wgan.png'.format(batch_size, image_size,num_epochs)\n",
    "    plt.savefig(file_name,dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa91fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 12:32:41.030100: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-15 12:32:41.056711: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 12:32:41.441550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /home/stud1/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 91.2M/91.2M [00:05<00:00, 17.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.0562), tensor(0.0167))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_noise_ = torch.randn(100, nz, 1, 1, device=device)\n",
    "imgs=fixed_noise\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "inception = InceptionScore()\n",
    "# generate some images\n",
    "imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "inception.update(imgs)\n",
    "inception.compute()\n",
    "\n",
    "\n",
    "# fake_ = netG(fixed_noise_).detach().cpu()\n",
    "# #fake_images_ = vutils.make_grid(fake_, normalize=True).numpy()\n",
    "# fake_images1000 = vutils.make_grid(fake_, normalize=True).numpy().transpose((1, 2, 0))  # Transpose to (H, W, C)\n",
    "# vutils.save_image(fake_, 'fake1000.png', normalize=True)\n",
    "# plt.imshow(fake_images1000)\n",
    "# #plt.show(fake_images_\\)\n",
    "# a = netD(fixed_noise_).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35451a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from scipy.linalg import sqrtm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calculate_fid(real_images, generated_images, batch_size, device, inception_batch_size=64):\n",
    "    \"\"\"\n",
    "    Calculates Fréchet Inception Distance (FID) score for GAN-generated images.\n",
    "\n",
    "    Args:\n",
    "        real_images (torch.Tensor): Real images from the real dataset.\n",
    "        generated_images (torch.Tensor): Generated images from the GAN model.\n",
    "        batch_size (int): Batch size for calculating FID score.\n",
    "        device (str): Device to run the calculations on ('cuda' or 'cpu').\n",
    "        inception_model (torch.nn.Module): Pre-trained Inception model.\n",
    "        inception_batch_size (int, optional): Batch size for calculating Inception features. Default is 64.\n",
    "\n",
    "    Returns:\n",
    "        float: FID score.\n",
    "    \"\"\"\n",
    "    # Set models to evaluation mode\n",
    "    inception_model = models.inception_v3(pretrained=True, transform_input=False).eval().to(device)\n",
    "    inception_model.eval()\n",
    "\n",
    "    # Move images to the specified device\n",
    "    real_images = real_images.to(device)\n",
    "    generated_images = generated_images.to(device)\n",
    "\n",
    "    # Resize and normalize images\n",
    "    real_images = adaptive_avg_pool2d(real_images, (299, 299))\n",
    "    real_images = (real_images - 0.5) * 2.0\n",
    "    generated_images = adaptive_avg_pool2d(generated_images, (299, 299))\n",
    "    generated_images = (generated_images - 0.5) * 2.0\n",
    "\n",
    "    # Calculate Inception features for real images\n",
    "    real_features = []\n",
    "    num_batches = real_images.size(0) // inception_batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch = real_images[i * inception_batch_size:(i + 1) * inception_batch_size]\n",
    "        real_features.append(inception_model(batch).detach().view(batch.size(0), -1))\n",
    "    real_features = torch.cat(real_features, dim=0)\n",
    "\n",
    "    # Calculate Inception features for generated images\n",
    "    generated_features = []\n",
    "    num_batches = generated_images.size(0) // inception_batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch = generated_images[i * inception_batch_size:(i + 1) * inception_batch_size]\n",
    "        generated_features.append(inception_model(batch).detach().view(batch.size(0), -1))\n",
    "    generated_features = torch.cat(generated_features, dim=0)\n",
    "\n",
    "#     # Calculate mean and covariance of real and generated features\n",
    "#     real_mean = torch.mean(real_features, dim=0)\n",
    "    # Move real features to CPU memory\n",
    "    real_features = real_features.detach().cpu()\n",
    "\n",
    "    # Calculate mean and covariance of real features\n",
    "    real_mean = torch.mean(real_features, dim=0)\n",
    "    real_cov = np.cov(real_features.T)\n",
    "    \n",
    "    generated_features = generated_features.detach().cpu()\n",
    "    generated_mean = torch.mean(generated_features, dim=0)\n",
    "    generated_cov = np.cov(generated_features.T)\n",
    "    #print(real_cov)\n",
    "    #print(generated_cov)\n",
    "\n",
    "    # Calculate FID score\n",
    "    mean_diff = real_mean - generated_mean\n",
    "    cov_mean_sqrt = sqrtm(real_cov.dot(generated_cov)).real\n",
    "    cov_mean_sqrt = torch.from_numpy(cov_mean_sqrt).to(device)\n",
    "    # Convert real_cov and generated_cov to PyTorch tensors\n",
    "    real_cov = torch.from_numpy(real_cov).to(device)\n",
    "    generated_cov = torch.from_numpy(generated_cov).to(device)\n",
    "\n",
    "    # Calculate FID score\n",
    "    fid = torch.sqrt(torch.sum(mean_diff * mean_diff) + torch.trace(real_cov + generated_cov - 2 * cov_mean_sqrt))\n",
    "\n",
    "    #fid = torch.sqrt(torch.sum(mean_diff * mean_diff) + torch.trace(real_cov + generated_cov - 2 * cov_mean_sqrt))\n",
    "\n",
    "    return fid.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "inception = InceptionScore()\n",
    "# generate some images\n",
    "imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "inception.update(imgs)\n",
    "inception.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load real images from data loader\n",
    "\n",
    "# real_images_iter = iter(dataloader)\n",
    "# real_images = next(real_images_iter)\n",
    "# real_images = real_images[0].to(device)\n",
    "\n",
    "# # Generate images using trained generator\n",
    "#   # Replace with your own generator\n",
    "# netG.eval()  # Set to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     generated_images = netG(torch.randn(real_images.size(0), nz,1,1).to(device))\n",
    "# generated_images = generated_images.cpu()  # Move to CPU if necessary\n",
    "\n",
    "# # Calculate FID score\n",
    "# fid_score = calculate_fid(real_images, generated_images,batch_size, inception_batch_size=64, device=device)\n",
    "\n",
    "# print(\"FID score: {:.4f}\".format(fid_score))\n",
    "# fid_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches = len(dataloader)\n",
    "# for batch_idx, real_images in enumerate(dataloader):\n",
    "#     for batch_idx in range(total_batches):\n",
    "#     real_images = next(iter(dataloader))\n",
    "#     real_images = real_images[0].to(device)\n",
    "for batch_idx in range(total_batches):\n",
    "    real_images = next(iter(dataloader))\n",
    "    real_images = real_images[0].to(device)\n",
    "\n",
    "\n",
    "    # Generate images using trained generator\n",
    "    # Replace with your own generator\n",
    "    netG.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        generated_images = netG(torch.randn(batch_size, nz, 1, 1).to(device))  # Generate batch_size images\n",
    "    generated_images = generated_images.cpu()  # Move to CPU if necessary\n",
    "\n",
    "    # Calculate FID score\n",
    "    fid_score = calculate_fid(real_images, generated_images, batch_size, inception_batch_size=64, device=device)\n",
    "\n",
    "    print(\"Batch {}/{} - FID score: {:.4f}\".format(batch_idx + 1, len(dataloader), fid_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1c355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
